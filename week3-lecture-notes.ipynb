{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "613d355a-0bc2-4c0b-ad12-4644f414e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scikit-learn==1.3.2 pandas==2.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a61ac-b598-4d94-9442-f0a47605a9f6",
   "metadata": {},
   "source": [
    "# Text representation, word embeddings, and sentence embeddings\n",
    "\n",
    "In this lecture, we will learn about text representations. In particular, we will go through a few examples of word embeddings and sentence embeddings. They are tools that have a wide variety of uses. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e8c3d-4885-4acb-94d2-b3dc04c163ec",
   "metadata": {},
   "source": [
    "## Text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbdd1c-9223-4210-b271-1ccf352753b5",
   "metadata": {},
   "source": [
    "<img src=\"images/week3-pnlp-01-pipeline.png\">\n",
    "\n",
    "representation of the NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8ae6f-aeeb-4291-bf30-3f112bb272a6",
   "metadata": {},
   "source": [
    "### Side topic: what is tokenization?\n",
    "\n",
    "A simple description of tokenization is that we need to split an English sentence into components -- usually defined by words, but sometimes other things. We will see an example later and in later lectures when we talk about modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64f4c8-5caf-4c44-ac4f-e830f6225c43",
   "metadata": {},
   "source": [
    "## Embeddings: current state\n",
    "\n",
    "At my job, I learned that when you are doing research and exploration, if you can throw a kitchen sink at a problem, you should. Use the latest tool to see if you can solve the problem before going deep into designing your own solutions.\n",
    "\n",
    "Let's produce some embeddings with `sentence_transfomer` from huggingface, then we will go back to the basics.\n",
    "\n",
    "See [hackerllama's notebook](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd133b3f-7ea1-4cfb-b1c2-f97564406bc6",
   "metadata": {},
   "source": [
    "## What problems are we trying to solve with embeddings?\n",
    "\n",
    "Text can not be consumed by computers as is -- we must find a numeric representation of our text in order to process them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347997b8-64ec-43b6-b949-8f52186c4013",
   "metadata": {},
   "source": [
    "We also have the following questions\n",
    "\n",
    "* How similar are two pieces of texts?\n",
    "* How can we find neighbors? For example, what are the texts most relevant to the one in question?\n",
    "    - semantic search (vector search) as opposed to keyword search\n",
    "    - top k neighbors = ranking\n",
    "\n",
    "By turning texts into vectors, we also get the following benefits:\n",
    "\n",
    "* We can do vector algebra on texts!\n",
    "* We can turn unstructured text data into structured feature for other models e.g. for predictive modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a1438-591f-4d88-a021-54f6536b5f0c",
   "metadata": {},
   "source": [
    "## Before sentence-transformers?\n",
    "\n",
    "Let's go from the beginning to see how we arrive at where we are today. Examples from here on are taken from the Practical NLP book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cbeee6-ce4f-4975-93e3-313570aefe29",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "One of the very first thought that comes to mind is, can we build a dictionary from the text, then encode our sentence with it?\n",
    "\n",
    "One way to think of this is to encode dummy variables in regression models. Each word is a category in the dummy variable.\n",
    "\n",
    "Challenge: what if we have tens of thousands of categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a5c5c1-9215-49f0-a9c6-cc7f8d979b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercased, punctuation removed text\n",
    "docs = [\n",
    "    'dog bites man',\n",
    "    'man bites dog',\n",
    "    'dog eats meat',\n",
    "    'man eats dog food',\n",
    "    # 'its sunny today',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b86ad73-80ec-4518-a0b6-067e202e1633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
     ]
    }
   ],
   "source": [
    "# taken from https://github.com/practical-nlp/practical-nlp-code/blob/master/Ch3/01_OneHotEncoding.ipynb\n",
    "vocab = {}\n",
    "count = 0\n",
    "for doc in docs:\n",
    "    for word in doc.split():\n",
    "        if word not in vocab:\n",
    "            count = count +1\n",
    "            vocab[word] = count\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ed4070-ce35-47fa-b5ae-6da102c5eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/practical-nlp/practical-nlp-code/blob/master/Ch3/01_OneHotEncoding.ipynb\n",
    "\n",
    "def get_onehot_vector(sentence):\n",
    "  onehot_encoded = []\n",
    "  for word in sentence.split():\n",
    "             temp = [0]*len(vocab)\n",
    "             if word in vocab:\n",
    "                        temp[vocab[word]-1] = 1\n",
    "             onehot_encoded.append(temp)\n",
    "  return onehot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7b54d-9b08-4b6d-885a-c2945668cba6",
   "metadata": {},
   "source": [
    "For each piece of text, you get a collection of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d218c6b-7b6c-482d-b3e5-593a0eb6a840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy variables\n",
    "get_onehot_vector(docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce90a4d8-bb06-4474-887e-f44a1fb8a3c9",
   "metadata": {},
   "source": [
    "### Immediate issues\n",
    "\n",
    "* The vocabulary set is fixed - it is determined by the document set\n",
    "* Vector length depending on the length of text **- bad for data storage**\n",
    "\n",
    "This is not very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c53f95e5-70f5-4da7-96cd-da535b346526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_onehot_vector(docs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc7ccdb-f846-4ce7-810c-206ada25da0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_onehot_vector(docs[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddb7eb-54ca-4c33-8f72-dca10053e01d",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    "\n",
    "We can take a step further from one-hot encoding and create a bag of words. This collapses each piece of text from a collection of vectors into a single vector.\n",
    "\n",
    "Personally, I found that they were less useful in applications because there are better methods. But let's take a quick example to see what Bag-of-words look like. We will skip the N-grams -- please refer to the book if you are interested, but it is essentially bag-of-words but with 2-grams or more to capture phrases and word relations instead of single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad7a31c-500f-4f53-8543-68e7be24d96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats dog food']\n",
      "Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
      "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Modified from https://github.com/practical-nlp/practical-nlp-code/blob/master/Ch3/02_Bag_of_Words.ipynb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#look at the documents list\n",
    "print(\"Our corpus: \", docs)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \", bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632148a4-6938-413a-81b6-2aa185b0172b",
   "metadata": {},
   "source": [
    "Note that these two printed out the exact same representation -- this approach does not take into the context at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21323e67-225c-4557-96d2-555e9f40ab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953b47f-c69d-465e-ad27-6716ebf233f4",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "TF-IDF stands for **term frequency–inverse document frequency.** \n",
    "\n",
    "It is a simple idea but still very powerful. This is still used heavily in keyword search and surfacing important keywords in a collection of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bd7f6-e952-4397-9add-dde760b363d2",
   "metadata": {},
   "source": [
    "<img src=\"images/week3-tf-idf-chris-albon.webp\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4756c43-4cbb-42fa-905b-f44335a4b551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [1.51082562 1.         1.51082562 1.91629073 1.22314355 1.91629073]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(docs)\n",
    "\n",
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4279953f-f93c-45aa-8c56-accbad496099",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Note that this line in the book Notebook throws an error - need to find the correct method using dir()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll words in the vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "# Note that this line in the book Notebook throws an error - need to find the correct method using dir()\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1a0ed58-8190-4f48-b34e-4f8009ed9a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words in the vocabulary ['bites' 'dog' 'eats' 'food' 'man' 'meat']\n",
      "----------\n",
      "TFIDF representation for all documents in our corpus\n",
      " [[0.69113141 0.4574528  0.         0.         0.55953044 0.        ]\n",
      " [0.69113141 0.4574528  0.         0.         0.55953044 0.        ]\n",
      " [0.         0.37919167 0.5728925  0.         0.         0.72664149]\n",
      " [0.         0.34399327 0.51971385 0.65919112 0.42075315 0.        ]]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names_out())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#TFIDF representation for all documents in our corpus \n",
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dedd2ba-84c9-4a51-a9cf-93a697d815dd",
   "metadata": {},
   "source": [
    "This still suffers from the same Out-of-Vocabulary problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4770c391-e534-404a-9ef6-7cb02d0940be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf representation for 'dog and man are friends':\n",
      " [[0.         0.63295194 0.         0.         0.77419109 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7033500-49a5-4a00-9664-a1bafb4e4cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf representation for 'dog and man are friends that play together':\n",
      " [[0.         0.63295194 0.         0.         0.77419109 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "temp = tfidf.transform([\"dog and man are friends that play together\"])\n",
    "print(\"Tfidf representation for 'dog and man are friends that play together':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6f3ce-5cd5-415d-a126-87e04d17f27d",
   "metadata": {},
   "source": [
    "##### Advantages of tf-idf\n",
    "\n",
    "* **Fast to compute**\n",
    "* Fits into human understanding and current use of text search well - Rare terms are more specific.\n",
    "\n",
    "What do I mean by rare terms are more specific? We can build a quick text search function by ranking the documents with the tf-idf score of our search term!\n",
    "\n",
    "In this example, searching for `Lego` would yield `The Lego Movie` with a TF-IDF score highere than searching for `love` with `Sleepless in Seattle`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e6f28-3681-4863-8eeb-7e39167cb70f",
   "metadata": {},
   "source": [
    "<img src=\"images/week3-tf-idf-search.png\">\n",
    "Screenshot from Chapter 3 of Relevant Search by Doug Turnbull and John Berryman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56860940-2701-4a8f-8a58-4765ddbdcc18",
   "metadata": {},
   "source": [
    "**PechaKucha candidate: What is BM25 scaling?**\n",
    "\n",
    "**PechaKucha candidate: What is Zipf's law?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585a4ad-951d-45c7-a746-637a1e111dbc",
   "metadata": {},
   "source": [
    "## Sparse embeddings vs dense embeddings\n",
    "\n",
    "In the beginning of class, we saw `sentence-transfomers` which produces a **dense embedding**.\n",
    "\n",
    "Bag-of-words and TF-IDF produce **sparse embeddings**\n",
    "\n",
    "In general, sparse embeddings are great for matching keywords. Dense embeddings are better at capturing **context**.\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "We will not go into how the Word2Vec algorithm was trained. Instead, let's play with the results.\n",
    "\n",
    "* JS demo for vector algebra: https://turbomaze.github.io/word2vecjson/\n",
    "* https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce31b99-cba5-4d43-8f76-8d6eaaeca541",
   "metadata": {},
   "source": [
    "## Transformer based embeddings\n",
    "\n",
    "In transformer based embeddings, we take the last hidden layer as the numerical represention of the text. Why does this work? We will learn more in the transformer lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa29a5-9e56-4096-9ca9-4b3207652c5a",
   "metadata": {},
   "source": [
    "## Finding neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b35963-537f-4123-a791-9b4492e9e42d",
   "metadata": {},
   "source": [
    "<img src=\"images/week3-cosine-similarity.png\">\n",
    "\n",
    "Formula for cosine similarity, a common measure of \"similarity\" for embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b957fde-1742-4a0b-aa95-08517288a58d",
   "metadata": {},
   "source": [
    "## Evaluating embeddings\n",
    "\n",
    "Before we go too far into work on embeddings, let's stop and think about: how do we evaluate embeddings? What are we looking at?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ca3aa-4705-4214-b4d3-1690e7c35c4c",
   "metadata": {},
   "source": [
    "Let's take a look at how the researchers generated a dataset for benchmarking algorithms\n",
    "\n",
    "**SimLex-999**\n",
    "\n",
    "* https://fh295.github.io//simlex.html\n",
    "* https://aclanthology.org/J15-4004.pdf\n",
    "\n",
    "This dataset contains 999 pairs of English words that are labeled by human on their similarity (but not relatedness) using Amazon Mechanical Turk. **Please take a look at their annotation guideline.**\n",
    "\n",
    "The researcher created a scale of 0-6 to calculate similary, then rescaled to 0-10 as similarity score.\n",
    "\n",
    "To reproduce this notebook, [download the dataset from here](https://fh295.github.io//SimLex-999.zip), unzip, then drop the `.csv` into the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b85d202-5368-48c7-9f27-f7b243dd49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9894d221-b9c4-41d9-a133-81ebba75cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simlex = pd.read_csv('data/SimLex-999.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ac3f45d-cbb6-4b46-89e0-88fd8a81a6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>POS</th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>conc(w1)</th>\n",
       "      <th>conc(w2)</th>\n",
       "      <th>concQ</th>\n",
       "      <th>Assoc(USF)</th>\n",
       "      <th>SimAssoc333</th>\n",
       "      <th>SD(SimLex)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>A</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>A</td>\n",
       "      <td>9.20</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>A</td>\n",
       "      <td>8.77</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word1        word2 POS  SimLex999  conc(w1)  conc(w2)  concQ  Assoc(USF)  \\\n",
       "0    old          new   A       1.58      2.72      2.81      2        7.25   \n",
       "1  smart  intelligent   A       9.20      1.75      2.46      1        7.11   \n",
       "2   hard    difficult   A       8.77      3.76      2.21      2        5.94   \n",
       "\n",
       "   SimAssoc333  SD(SimLex)  \n",
       "0            1        0.41  \n",
       "1            1        0.67  \n",
       "2            1        1.19  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_simlex.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86bc0b04-8137-4c33-bcef-845aa40c1655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>POS</th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>conc(w1)</th>\n",
       "      <th>conc(w2)</th>\n",
       "      <th>concQ</th>\n",
       "      <th>Assoc(USF)</th>\n",
       "      <th>SimAssoc333</th>\n",
       "      <th>SD(SimLex)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>butter</td>\n",
       "      <td>potato</td>\n",
       "      <td>N</td>\n",
       "      <td>1.22</td>\n",
       "      <td>4.90</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>choose</td>\n",
       "      <td>elect</td>\n",
       "      <td>V</td>\n",
       "      <td>7.62</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>bread</td>\n",
       "      <td>flour</td>\n",
       "      <td>N</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.92</td>\n",
       "      <td>4.97</td>\n",
       "      <td>4</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1   word2 POS  SimLex999  conc(w1)  conc(w2)  concQ  Assoc(USF)  \\\n",
       "453  butter  potato   N       1.22      4.90      4.85      4        0.27   \n",
       "793  choose   elect   V       7.62      2.62      2.41      1        1.28   \n",
       "209   bread   flour   N       3.33      4.92      4.97      4        1.42   \n",
       "\n",
       "     SimAssoc333  SD(SimLex)  \n",
       "453            0        1.19  \n",
       "793            1        1.14  \n",
       "209            1        1.25  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_simlex.sample(3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92bf7f09-4a8c-44a1-a4ab-ff98971a6986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>conc(w1)</th>\n",
       "      <th>conc(w2)</th>\n",
       "      <th>Assoc(USF)</th>\n",
       "      <th>SD(SimLex)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.561572</td>\n",
       "      <td>3.657087</td>\n",
       "      <td>3.568629</td>\n",
       "      <td>0.751512</td>\n",
       "      <td>1.274505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.614663</td>\n",
       "      <td>1.131050</td>\n",
       "      <td>1.159572</td>\n",
       "      <td>1.344569</td>\n",
       "      <td>0.366278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.230000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.380000</td>\n",
       "      <td>2.620000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>1.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.670000</td>\n",
       "      <td>3.830000</td>\n",
       "      <td>3.660000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.750000</td>\n",
       "      <td>4.790000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>1.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.800000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.850000</td>\n",
       "      <td>2.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SimLex999    conc(w1)    conc(w2)  Assoc(USF)  SD(SimLex)\n",
       "count  999.000000  999.000000  999.000000  999.000000  999.000000\n",
       "mean     4.561572    3.657087    3.568629    0.751512    1.274505\n",
       "std      2.614663    1.131050    1.159572    1.344569    0.366278\n",
       "min      0.230000    1.190000    1.190000    0.000000    0.340000\n",
       "25%      2.380000    2.620000    2.500000    0.140000    1.075000\n",
       "50%      4.670000    3.830000    3.660000    0.250000    1.310000\n",
       "75%      6.750000    4.790000    4.750000    0.680000    1.540000\n",
       "max      9.800000    5.000000    5.000000    8.850000    2.180000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_simlex.describe(include='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d2d0879-418a-4745-a1f6-db56f0df5062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>POS</th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>conc(w1)</th>\n",
       "      <th>conc(w2)</th>\n",
       "      <th>concQ</th>\n",
       "      <th>Assoc(USF)</th>\n",
       "      <th>SimAssoc333</th>\n",
       "      <th>SD(SimLex)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>A</td>\n",
       "      <td>9.20</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>A</td>\n",
       "      <td>9.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>happy</td>\n",
       "      <td>glad</td>\n",
       "      <td>A</td>\n",
       "      <td>9.17</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.36</td>\n",
       "      <td>1</td>\n",
       "      <td>5.49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stupid</td>\n",
       "      <td>dumb</td>\n",
       "      <td>A</td>\n",
       "      <td>9.58</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.36</td>\n",
       "      <td>1</td>\n",
       "      <td>5.26</td>\n",
       "      <td>1</td>\n",
       "      <td>1.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>insane</td>\n",
       "      <td>crazy</td>\n",
       "      <td>A</td>\n",
       "      <td>9.57</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word1        word2 POS  SimLex999  conc(w1)  conc(w2)  concQ  Assoc(USF)  \\\n",
       "1    smart  intelligent   A       9.20      1.75      2.46      1        7.11   \n",
       "3    happy     cheerful   A       9.55      2.56      2.34      1        5.85   \n",
       "6    happy         glad   A       9.17      2.56      2.36      1        5.49   \n",
       "8   stupid         dumb   A       9.58      1.75      2.36      1        5.26   \n",
       "16  insane        crazy   A       9.57      1.77      2.37      1        2.09   \n",
       "\n",
       "    SimAssoc333  SD(SimLex)  \n",
       "1             1        0.67  \n",
       "3             1        2.18  \n",
       "6             1        1.59  \n",
       "8             1        1.48  \n",
       "16            1        0.92  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_simlex.query('SimLex999 > 9.0').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ccbf4-2a03-44e7-a87b-c0e56f09c9a4",
   "metadata": {},
   "source": [
    "Problem with evaluation: \n",
    "\n",
    "Words are context dependent. big, large, huge can have different meanings in different context. Think about the following sentences:\n",
    "\n",
    "* My big brother\n",
    "* My large brother\n",
    "* My huge brother\n",
    "\n",
    "But their (human) similarity score was high -- we must consider the downstream task when evaluating similarities. More to come"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e38352-3d5c-4347-9495-c0a271b7fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simlex[lambda df: df['word1'] == 'large']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8294a-942c-4cc1-93ef-357c76f6d99f",
   "metadata": {},
   "source": [
    "**Remember this for the project!!** How you evaluate is often more important than what you did."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
